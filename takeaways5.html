<!DOCTYPE html>
<html lang="en">

<head>
<meta charset="utf-8" />
<link rel="stylesheet" href="https://dokie.li/media/css/lncs.css" />
<title>Takeaways for Week 5</title>
</head>

<body>

<h1>Takeaways for Week 5</h1>

<h2>Topics</h2>

<h3>Knowledge Representation on the Web</h3>

<p class="counter">
The web has a number of properties, from which a number of challenges arise. Some of these properties are 1) open: the web can be accessed by everyone and every one can contribute to it, 2: decentralized: the web is not controlled by one authority, 3)structured by links: where links can be made across pages on the web. A number of challenges arise because of these properties, including the fact that data of low-quality is present on the web, and users can not be controlled about the content they add. This leads to having the same concept defined multiple times, by different users, including users with no technical background. In order to organise the content and knowledge on the web, some standards and languages are used on the Semantic web which is a part of the Web. These languages are RDF, SPARQL, OWL (ontologies), which provide standards and semantic principles. 
</p>

<h3>Knowledge Modeling</h3>

<p class="counter">
In order to extract knowledge from peoples’ minds, we need first to perform the process of knowledge modeling, which is a key task in knowledge organization. An example of knowledge representation is categorization of which three types are defined: individual, cultural, and institutional categorization. Individual categorization occurs when a person creates his/her own list of categories for his/her own use, e.g. assigning emails to different folders. Cultural categorization deals with the categorization process happening in a whole culture/society, e.g. values shared by a culture. Institutional categorization serves institutional goals, e.g. concepts used by an institution. Knowledge modeling is about making decisions, and it is not easy to do. Knowledge Acquisition Bottleneck is about the process of translating knowledge from the expert to the user, where an interaction is needed between the expert and the user to support knowledge modelling. 
</p>

<h2>Literature</h2>

<h3>Bizer et al. 2009</h3>

<p class="counter">
There are a number of knowledge bases that allow information extraction, but they are all made by small groups and allow information extraction in one domain, or in specific domains only. However, DBpedia is a knowledge base that allows information extraction in many domains, and in many languages. Bizer et al (2009) introduce a description of the data construction platform DBpedia and its extraction. DBpedia is dependent on the information that is available on Wikipedia, where Wikipedia is the information resource from which DBpedia extracts structured information, and makes it available on the web. DBpedia contains a huge number of RDF triples (274 million RDF triples) which are extracted from Wikipedia in many languages, and this number evolves automatically as Wikipedia evolves. There are four access mechanisms through which DBpedia can be accessed: 1) linked data, SPARQL endpoint, RDF dumps, or Lookup index. Many applications exploit the benefits of DBpedia which provides information about a wide range of domains, and it also facilitates automated annotations. 
</p>

<h3>Vrandecic and Krotzsch 2014</h3>

<p class="counter">
Vrandecic and Krotzsch (2014) introduce a description of the free collaborative knowledgebase Wikidata, which is considered to be the sister project of Wikipedia. The purpose of Wikidata is to manage, clean and integrate Wikipedia’s data. A number of design decisions were taken when Wikidata was designed and established: 1) open editing: every user can edit the information, 2) community control: data and data schema is controlled by the contributor community, 3) plurality: allowance for conflicting data to coexist, 4) secondary data: data is published with its resource, 5) multilingual data: data in translated into many different languages, 6) easy access: data is provided in different formats to allow usability in a wide range, 7) continuous evolution: Wikidata evolves as the number of its editors evolve. 
</p>

<h3>Beek et al. 2016</h3>

<p class="counter">
Beek et al (2016) introduce LOD Laundromat as a solution for the Semantic Web problems, which include the Semantic Web not being machine readable any more, nor is it traversable by Web agents, and the information not being accessed easily. These problems make the Semantic Web contradict the principles of the original Web of data. However, LOD Laundromat makes the Semantic Web be closer to the principles of the original Web of data. By implementing the LOD Laundromat, some of the Semantic Web assumptions are dropped off (distribution, reuse, and navigation). LOD Laundromat cleans the data from any syntax error, which prevents the document from being machine readable. Also, LOD Laundromat allows for downloading the cleaned data, and lowers the cost for data consumers by centralizing the data. Beek et al (2016) consider the SPARQL endpoint unsuitable for querying the data from the Semantic Web. Therefore, they developed the Federated Resource Architecture for Networked Knowledge (Frank), which makes it possible to query the data from the Semantic Web from the command line. 
</p>

<h3>Schreiber et al. 2008</h3>

<p class="counter">
Schreiber et al. (2008) describe the MultimediaN E-Culture Semantic Web application, which is an open source search software. The application  provides support in the search experience within a large number of virtual cultural-heritage resources, and it allows for better indexing of the virtual cultural-heritage resources. E-Culture’s main goal was to show that inter-collection search with low cost is possible by exploiting the features of the Semantic Web. In order to prove this, Schreiber et al. (2008) applied an approach of three main elements. One of these elements is the process of harvesting, enriching, and the alignment of metadata collections and vocabularies. This is done by translating the ontologies to RDF/OWL, then the metadata schema is aligned, after that the metadata is enriched to find matching concepts between the ontology and the demonstrator, and finally the vocabulary should also be aligned. As a result of this harvesting process, a graph of knowledge is produced which allows for easier and better search results when performing a query on multiple vocabularies at the same time. 
</p>

<h2>References</h2>

<ul>
<li>Bizer et al. 2009. <a href="http://dx.doi.org/10.1016/j.websem.2009.07.002">DBpedia - A crystallization point for the Web of Data.</a> Web Semantics, 7(3).</li>
<li>Vrandecic and Krotzsch. 2014. <a href="http://dx.doi.org/10.1145/2629489">Wikidata: a free collaborative knowledgebase.</a> Communications of the ACM 57(10).</li>
<li>Beek et al. 2016. <a href="http://dx.doi.org/10.1109/mic.2016.43">LOD Laundromat: Why the Semantic Web Needs Centralization (Even If We Don’t Like It).</a> IEEE Internet Computing, 20(2).</li>
<li>Schreiber et al. 2008. <a href="http://dx.doi.org/10.1016/j.websem.2008.08.001">Semantic annotation and search of cultural-heritage collections: The MultimediaN E-Culture demonstrator.</a> Web Semantics, 6(4).</li>
</ul>

<script src="https://raw.githack.com/ucds-vu/ko2021-templates/main/scripts.js"></script>

</body>
</html>

